---
layout: default
---

I am Sakae Mizuki (水木 栄), Ph.D. graduate in Computer Science (Natural Language Processing) from Institute of Science Tokyo (formerly Tokyo Institute of Technology), where I had the honor of being supervised by [Prof. Naoaki Okazaki](http://www.chokkan.org/index.en.html). Having completed my doctoral studies, I now continue my career in the R&D division at Hotto Link, Inc.

In parallel to my full-time work, I am also a part-time researcher at [Institute of Science Tokyo](https://www.nlp.c.titech.ac.jp/index.en.html) and visiting researcher at [National Institute of Advanced Industrial Science and Technology](https://www.airc.aist.go.jp/). My research interests focused on representation learning, lexical semantics, with a specific emphasis on integrating lexical knowledge into large language models.

Github: [s-mizuki-nlp](https://github.com/s-mizuki-nlp)

Email: sakae.mizuki [aatt] nlp [dot] c.titech.ac.jp

# Publications

## Journal
* <u>Sakae Mizuki</u> and Naoaki Okazaki. Learning Hierarchical Code Representation for Hypernymy Detection. Journal of Information Processing (TOD), 14(4):8–23, 2021. [Paper](http://id.nii.ac.jp/1001/00213162/)

## International Conference
* Koshiro Saito, <u>Sakae Mizuki</u>, Masanari Ohi, Taishi Nakamura, Taihei Shiotani, Koki Maeda, Youmi Ma, Kakeru Hattori, Kazuki Fujii, Takumi Okamoto, Shigeki Ishida, Hiroya Takamura, Rio Yokota, and Naoaki Okazaki. Why We Build Local Large Language Models: An Observational Analysis from 35 Japanese, Multilingual LLMs. In The 1st Workshop on Multilingual and Equitable Language Technologies (MELT), October 2025. [Paper](https://arxiv.org/abs/2412.14471)
* Youmi Ma, <u>Sakae Mizuki</u>, Kazuki Fujii, Taishi Nakamura, Masanari Ohi, Hinari Shimada, Taihei Shiotani, Koshiro Saito, Koki Maeda, Kakeru Hattori, Takumi Okamoto, Shigeki Ishida, Rio Yokota, Hiroya Takamura, and Naoaki Okazaki. Building Instruction-Tuning Datasets from Human-Written Instructions with Open-Weight Large Language Models. In Proceedings of the Second Conference on Language Modeling (COLM 2025), October 2025. [Paper](https://arxiv.org/abs/2503.23714)
* Naoaki Okazaki, Kakeru Hattori, Hirai Shota, Hiroki Iida, Masanari Ohi, Kazuki Fujii, Taishi Nakamura, Mengsay Loem, Rio Yokota, <u>Sakae Mizuki</u>. Building a Large Japanese Web Corpus for Large Language Models. In Proceedings of the First Conference on Language Modeling (COLM 2024), October 2024. [Paper](https://arxiv.org/abs/2404.17733)
* Kazuki Fujii, Taishi Nakamura, Mengsay Loem, Hiroki Iida, Masanari Ohi, Kakeru Hattori, Hirai Shota, <u>Sakae Mizuki</u>, Rio Yokota, Naoaki Okazaki. Continual Pre-Training for Cross-Lingual LLM Adaptation: Enhancing Japanese Language Capabilities. Proceedings of the First Conference on Language Modeling (COLM 2024), October 2024. [Paper](https://arxiv.org/abs/2404.17790)
* Shogo Matsuno, <u>Sakae Mizuki</u>, and Takeshi Sakaki. Construction of Evaluation Datasets for Trend Forecasting Studies. In Proceedings of the 17th International AAAI Conference on Web and Social Media (ICWSM 2023), pp. 1041-1051, June 2023. [Paper](https://ojs.aaai.org/index.php/ICWSM/issue/view/532), [Dataset](https://zenodo.org/record/7014424)
* <u>Sakae Mizuki</u> and Naoaki Okazaki. Semantic Specialization for Knowledge-based Word Sense Disambiguation. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics (EACL 2023), pp. 3457-3470, May 2023. [Paper](https://aclanthology.org/2023.eacl-main.251/), [Code](https://github.com/s-mizuki-nlp/semantic_specialization_for_wsd)
* <u>Sakae Mizuki</u> and Naoaki Okazaki. Analyzing the Variation Property of Contextualized Word Representations. In AI 2019: Advances in Artificial Intelligence, pp. 393–405, December 2019. [Paper](https://link.springer.com/chapter/10.1007/978-3-030-35288-2_32)

## Domestic Conference
* Kakeru Hattori, Naoaki Okazaki, <u>Sakae Mizuki</u>, ..., and Hiroya Takamura. Swallow Corpus v2: Educational Japanese Web Text Corpus (Swallowコーパスv2: 教育的な日本語ウェブコーパスの構築). The 31st Annual Meeting of The Association for Natural Language Processing (NLP2025), March 2025. (in Japanese) [Paper](https://www.anlp.jp/proceedings/annual_meeting/2025/pdf_dir/C1-5.pdf)
* Youmi Ma, <u>Sakae Mizuki</u>, Kazuki Fujii, ..., and Naoaki Okazaki. Instruction Tuning of Large Language Models through Model Imitation (模倣学習による大規模言語モデルの指示チューニング). The 31st Annual Meeting of The Association for Natural Language Processing (NLP2025), March 2025. (in Japanese) [Paper](https://www.anlp.jp/proceedings/annual_meeting/2025/pdf_dir/Q8-21.pdf)
* Kakeru Hattori, <u>Sakae Mizuki</u>, Kazuki Fujii, ..., and Naoaki Okazaki. Developing a Japanese LLM Enhanced for Current Affairs and Society by Leveraging Newspaper Articles (新聞記事からつくる 時事と社会に強い日本語LLM). The 31st Annual Meeting of The Association for Natural Language Processing (NLP2025), March 2025. (in Japanese) [Paper](https://www.anlp.jp/proceedings/annual_meeting/2025/pdf_dir/C10-1.pdf)
* Koshiro Saito, <u>Sakae Mizuki</u>, Masanari Ohi, ..., and Naoaki Okazaki. Advantages of Training LLMs on Japanese Text (LLMに日本語テキストを学習させる意義). The 261st Special Interest Group on Natural Language Processing (SIG-NL), September 2024. (in Japanese) [Paper](https://ipsj.ixsq.nii.ac.jp/ej/?action=pages_view_main&active_action=repository_view_main_item_detail&item_id=238505&item_no=1&page_id=13&block_id=8), [Best Paper Award](https://sites.google.com/sig-nl.ipsj.or.jp/sig-nl/%E6%8E%88%E8%B3%9E/outstanding)
* Kazuki Fujii, Taishi Nakamura, Mengsay Loem, ..., <u>Sakae Mizuki</u>, Rio Yokota, and Naoaki Okazaki. Developing High-Performance Japanese Large-scale Language Models Using Continual Pre-training (継続事前学習による日本語に強い大規模言語モデルの構築). The 30th Annual Meeting of The Association for Natural Language Processing (NLP2024), March 2024. (in Japanese) [Paper](https://www.anlp.jp/proceedings/annual_meeting/2024/pdf_dir/A8-5.pdf), [Best Paper Award](https://www.anlp.jp/nlp2024/award.html)
* <u>Sakae Mizuki</u>, Hiroki Iida, Kazuki Fujii, ..., and Naoaki Okazaki. Efficient Japanese Language Capabilities Enhancement in Large-scale Language Models: Utilizing Vocabulary Expansion and Parallel Corpus in Continual Pre-training (大規模言語モデルの日本語能力の効率的な強化: 継続事前学習における語彙拡張と対訳コーパスの活用). The 30th Annual Meeting of The Association for Natural Language Processing (NLP2024), March 2024. (in Japanese) [Paper](https://www.anlp.jp/proceedings/annual_meeting/2024/pdf_dir/A6-4.pdf)
* Naoaki Okazaki, Kakeru Hattori, Shota Hirai, ..., and <u>Sakae Mizuki</u>. Swallow Corpus: Large-scale Japanese Web Text Corpus (Swallowコーパス: 日本語大規模ウェブコーパス). The 30th Annual Meeting of The Association for Natural Language Processing (NLP2024), March 2024. (in Japanese) [Paper](https://www.anlp.jp/proceedings/annual_meeting/2024/pdf_dir/A6-1.pdf), [Best Paper Award](https://www.anlp.jp/nlp2024/award.html)
* <u>Sakae Mizuki</u> and Naoaki Okazaki. Semantic Specialization for Knowledge-based Word Sense Disambiguation. 29th Annual Meeting of The Association for Natural Language Processing (NLP2023), March 2023. (in Japanese) [Paper](https://www.anlp.jp/proceedings/annual_meeting/2023/pdf_dir/C3-1.pdf), [Best Paper Award](https://www.anlp.jp/nlp2023/award.html)
* Ryogo Ishikawa, Ayana Niwa, <u>Sakae Mizuki</u>, Naoaki Okazaki. Robust Dependency Parsing for the Omission of a Post-positional Particle using Pseudo Training Data. The 28th Annual Meeting of the Association for Natural Language Processing (NLP2022), March 2022. (in Japanese) [Paper](https://www.anlp.jp/proceedings/annual_meeting/2022/pdf_dir/B7-1.pdf)
* <u>Sakae Mizuki</u> and Naoaki Okazaki. Hyponymy Detection using Hierarchical Code Learning. 27th Annual Meeting of The Association for Natural Language Processing (NLP2021), pp. 1236–1241, March 2021. (in Japanese) [Paper](https://www.anlp.jp/proceedings/annual_meeting/2021/pdf_dir/A7-4.pdf), [Best Paper Award](https://www.anlp.jp/nlp2021/award.html)
* <u>Sakae Mizuki</u> and Takeshi Sakaki. General-Purpose Oriented Extended Named Entity Labeling of Wikipedia Entries. IEICE Technical Report, vol. 117, no. 82, NLC2017-9, pp. 47-52, June 2017. (in Japanese) [Paper](https://ken.ieice.org/ken/paper/20170610FbuG/eng/), [Best Paper Award](https://www.ieice.org/iss/nlc/wiki/wiki.cgi?page=%B8%A6%B5%E6%BE%DE2017%C7%AF%C8%EF%C9%BD%BE%B4%BC%D4)

## Articles
* Sakae Mizuki. Behind the Scenes of "Semantic Specilization for Knowledge-based Word Sense Disambiguation" (「埋め込み表現の意味適応による知識ベース語義曖昧性解消」ができるまで). Journal of Natural Language Processing, vol. 30, no. 3, pp. 1105-1109, September 2023. (in Japanese) [Article](https://www.jstage.jst.go.jp/article/jnlp/30/3/30_1105/_article/-char/ja)

## Others
* For a full list of publications including research papers written as part of office duties, refer to my [Google Scholar](https://scholar.google.co.jp/citations?user=ryX_tAEAAAAJ&hl=ja)

# Honors and Awards
* Sep. 2024: Best Paper Award (co-authored), the 261st Special Interest Group on Natural Language Processing (SIG-NL)
* Mar. 2024: Best Paper Award (co-authored two papers), the 30th Annual Meeting of The Association for Natural Language Processing (NLP2024)
* Mar. 2023: Best Paper Award, the 29th Annual Meeting of The Association for Natural Language Processing (NLP2023)
* Mar. 2021: Best Paper Award, the 27th Annual Meeting of The Association for Natural Language Processing (NLP2021)
* Jan. 2018: Best Paper Award, IEICE Technical Committee on Natural Language Understanding and Models of Communication (NLC)

# Education
* Apr. 2018 - Dec. 2023: Ph.D. of Engineering (Computer Science), Institute of Science Tokyo (formerly Tokyo Institute of Technology), Japan
    * Research field: Natural Language Processing
* Apr. 2007 - Mar. 2009: Master of Engineering, Tokyo University, Japan
    * Research field: Aerospace Engineering
* Apr. 2003 - Mar. 2007: Bachelor of Engineering, Nagoya University, Japan
    * Research field: Aerospace Engineering

# Work Experience
* Jul. 2024 - Present: National Institute of Advanced Industrial Science and Technology
    * Visiting Researcher
* Jan. 2024 - Present: Institute of Science Tokyo
    * Part-time Researcher
* Mar. 2015 - Present: Hotto Link Inc.
    * Research Engineer
* Apr. 2009 - Feb. 2015: Mizuho Bank, Ltd. (seconded to [Mizuho-DL Financial Technology Co., Ltd.](https://www.mizuhobank.co.jp/fintec/index.html))
    * Financial Engineer

# Projects
* Oct. 2023 - Present: [Swallow LLM: large-scale language model research and development program](https://swallow-llm.github.io/index.en.html)

# Talks
* Aug. 2024: Paper Reading Seminar. Instruction-tuned Language Models are Better Knowledge Learners. In: [最先端NLP勉強会 2024](https://sites.google.com/view/snlp-jp/home/2024). [Slide](https://speakerdeck.com/s_mizuki_nlp/instruction-tuned-language-models-are-better-knowledge-learners-in-acl-2024)
* Jan. 2024: LLM Seminar. 大規模言語モデルSwallow. In: [LLM勉強会](https://llm-jp.nii.ac.jp/resources/).
* Sep. 2023: LLM Seminar. Model imitationによるInstruction tuningのサーベイ. In: [LLM勉強会](https://llm-jp.nii.ac.jp/resources/).
* Mar. 2023: Invited talk. 埋め込み表現の意味適応による知識ベース語義曖昧性解消. In: [NLP Colloquium](https://nlp-colloquium-jp.github.io/).
* Mar. 2022: Workshop Lightning Talk. SNSを出典とする言語資源の公開にまつわるノウハウ. In: NLP2022 Co-located Workshop [日本語における評価用データセットの構築と利用性の向上](https://jedworkshop.github.io/jed2022/) (JED2022).
* Sep. 2021: Paper Reading Seminar. A Distributional Approach to Controlled Text Generation. In: [最先端NLP勉強会 2021](https://sites.google.com/view/snlp-jp/home/2021). [Slide](https://speakerdeck.com/s_mizuki_nlp/lun-wen-du-mihui-snlp2021-a-distributional-approach-to-controlled-text-generation)
* Sep. 2020: Paper Reading Seminar. ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators. In: [最先端NLP勉強会 2020](https://sites.google.com/view/snlp-jp/home/2020). [Slide](https://speakerdeck.com/s_mizuki_nlp/lun-wen-du-mihui-snlp2020-electra-pre-training-text-encoders-as-discriminators-rather-than-generators)
* Sep. 2019: Paper Reading Seminar. Ordered neurons: Integrating tree structures into recurrent neural networks. In: [最先端NLP勉強会 2019](https://sites.google.com/view/snlp-jp/home/2019). [Slide](https://speakerdeck.com/s_mizuki_nlp/lun-wen-du-mihui-snlp2019-ordered-neurons-integrating-tree-structures-into-recurrent-neural-networks)
* Aug. 2018: Paper Reading Seminar. Sequence-to-Action: End-to-End Semantic Graph Generation for Semantic Parsing. In: [最先端NLP勉強会 2018](https://sites.google.com/view/snlp-jp/home/2018). [Slide](https://speakerdeck.com/s_mizuki_nlp/lun-wen-du-mihui-snlp2018-sequence-to-action-end-to-end-semantic-graph-generation-for-semantic-parsing)
* Mar. 2016: Workshop. ソーシャルメディア分析サービスにおけるNLPに関する諸問題について. In: NLP2016 Co-located Workshop 論文に書かない（書けない）自然言語処理. 

# Membership
* The Association for Natural Language Processing (NLP)
* Information Processing Society of Japan (IPSJ)
* The Securities Analysts Association of Japan (SAAJ)
* [LLM-jp: LLM勉強会](https://llm-jp.nii.ac.jp/)

# Skills
* Japanese (native)
* English (fluent)
* Python, PyTorch (, R and C++)
* Elasticsearch
* Data science
