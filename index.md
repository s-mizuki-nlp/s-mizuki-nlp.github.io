---
layout: default
---

I am Sakae Mizuki (水木 栄), Ph.D. graduate in Computer Science (Natural Language Processing) from Tokyo Institute of Technology, where I had the honor of being supervised by [Prof. Naoaki Okazaki](http://www.chokkan.org/index.en.html). Having completed my doctoral studies, I now continue my career in the R&D division at Hotto Link, Inc.

In parallel to my full-time work, I am also a part-time researcher at [Tokyo Institute of Technology](https://www.nlp.c.titech.ac.jp/index.en.html) and visiting researcher at [National Institute of Advanced Industrial Science and Technology](https://www.airc.aist.go.jp/). My research interests focused on representation learning, lexical semantics, with a specific emphasis on integrating lexical knowledge into large language models.

Github: [s-mizuki-nlp](https://github.com/s-mizuki-nlp)

Email: sakae.mizuki [aatt] nlp [dot] c.titech.ac.jp

# Publications

## Journal
* <u>Sakae Mizuki</u> and Naoaki Okazaki. Learning Hierarchical Code Representation for Hypernymy Detection. Journal of Information Processing (TOD), 14(4):8–23, 2021. [Paper](http://id.nii.ac.jp/1001/00213162/)

## International Conference
* Naoaki Okazaki, Kakeru Hattori, Hirai Shota, Hiroki Iida, Masanari Ohi, Kazuki Fujii, Taishi Nakamura, Mengsay Loem, Rio Yokota, <u>Sakae Mizuki</u>. Building a Large Japanese Web Corpus for Large Language Models. In Proceedings of the First Conference on Language Modeling (COLM 2024), pp. (to appear), October 2024. [Paper](https://arxiv.org/abs/2404.17733)
* Kazuki Fujii, Taishi Nakamura, Mengsay Loem, Hiroki Iida, Masanari Ohi, Kakeru Hattori, Hirai Shota, <u>Sakae Mizuki</u>, Rio Yokota, Naoaki Okazaki. Continual Pre-Training for Cross-Lingual LLM Adaptation: Enhancing Japanese Language Capabilities. Proceedings of the First Conference on Language Modeling (COLM 2024), pp. (to appear), October 2024. [Paper](https://arxiv.org/abs/2404.17790)
* Shogo Matsuno, <u>Sakae Mizuki</u>, and Takeshi Sakaki. Construction of Evaluation Datasets for Trend Forecasting Studies. In Proceedings of the 17th International AAAI Conference on Web and Social Media (ICWSM 2023), pp. 1041-1051, June 2023. [Paper](https://ojs.aaai.org/index.php/ICWSM/issue/view/532), [Dataset](https://zenodo.org/record/7014424)
* <u>Sakae Mizuki</u> and Naoaki Okazaki. Semantic Specialization for Knowledge-based Word Sense Disambiguation. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics (EACL 2023), pp. 3457-3470, May 2023. [Paper](https://aclanthology.org/2023.eacl-main.251/), [Code](https://github.com/s-mizuki-nlp/semantic_specialization_for_wsd)
* <u>Sakae Mizuki</u> and Naoaki Okazaki. Analyzing the Variation Property of Contextualized Word Representations. In AI 2019: Advances in Artificial Intelligence, pp. 393–405, December 2019. [Paper](https://link.springer.com/chapter/10.1007/978-3-030-35288-2_32)

## Domestic Conference
* Kazuki Fujii, Taishi Nakamura, Mengsay Loem, ..., <u>Sakae Mizuki</u>, Rio Yokota, and Naoaki Okazaki. Developing High-Performance Japanese Large-scale Language Models Using Continual Pre-training (継続事前学習による日本語に強い大規模言語モデルの構築). The 30th Annual Meeting of The Association for Natural Language Processing (NLP2024), March 2024. (in Japanese) [Paper](https://www.anlp.jp/proceedings/annual_meeting/2024/pdf_dir/A8-5.pdf), [Best Paper Award](https://www.anlp.jp/nlp2024/award.html)
* <u>Sakae Mizuki</u>, Hiroki Iida, Kazuki Fujii, ..., and Naoaki Okazaki. Efficient Japanese Language Capabilities Enhancement in Large-scale Language Models: Utilizing Vocabulary Expansion and Parallel Corpus in Continual Pre-training (大規模言語モデルの日本語能力の効率的な強化: 継続事前学習における語彙拡張と対訳コーパスの活用). The 30th Annual Meeting of The Association for Natural Language Processing (NLP2024), March 2024. (in Japanese) [Paper](https://www.anlp.jp/proceedings/annual_meeting/2024/pdf_dir/A6-4.pdf)
* Naoaki Okazaki, Kakeru Hattori, Shota Hirai, ..., and <u>Sakae Mizuki</u>. Swallow Corpus: Large-scale Japanese Web Text Corpus (Swallowコーパス: 日本語大規模ウェブコーパス). The 30th Annual Meeting of The Association for Natural Language Processing (NLP2024), March 2024. (in Japanese) [Paper](https://www.anlp.jp/proceedings/annual_meeting/2024/pdf_dir/A6-1.pdf), [Best Paper Award](https://www.anlp.jp/nlp2024/award.html)
* <u>Sakae Mizuki</u> and Naoaki Okazaki. Semantic Specialization for Knowledge-based Word Sense Disambiguation. 29th Annual Meeting of The Association for Natural Language Processing (NLP2023), March 2023. (in Japanese) [Paper](https://www.anlp.jp/proceedings/annual_meeting/2023/pdf_dir/C3-1.pdf), [Best Paper Award](https://www.anlp.jp/nlp2023/award.html)
* Ryogo Ishikawa, Ayana Niwa, <u>Sakae Mizuki</u>, Naoaki Okazaki. Robust Dependency Parsing for the Omission of a Post-positional Particle using Pseudo Training Data. The 28th Annual Meeting of the Association for Natural Language Processing (NLP2022), March 2022. (in Japanese) [Paper](https://www.anlp.jp/proceedings/annual_meeting/2022/pdf_dir/B7-1.pdf)
* <u>Sakae Mizuki</u> and Naoaki Okazaki. Hyponymy Detection using Hierarchical Code Learning. 27th Annual Meeting of The Association for Natural Language Processing (NLP2021), pp. 1236–1241, March 2021. (in Japanese) [Paper](https://www.anlp.jp/proceedings/annual_meeting/2021/pdf_dir/A7-4.pdf), [Best Paper Award](https://www.anlp.jp/nlp2021/award.html)
* <u>Sakae Mizuki</u> and Takeshi Sakaki. General-Purpose Oriented Extended Named Entity Labeling of Wikipedia Entries. IEICE Technical Report, vol. 117, no. 82, NLC2017-9, pp. 47-52, June 2017. (in Japanese) [Paper](https://ken.ieice.org/ken/paper/20170610FbuG/eng/), [Best Paper Award](https://www.ieice.org/iss/nlc/wiki/wiki.cgi?page=%B8%A6%B5%E6%BE%DE2017%C7%AF%C8%EF%C9%BD%BE%B4%BC%D4)

## Articles
* Sakae Mizuki. Behind the Scenes of "Semantic Specilization for Knowledge-based Word Sense Disambiguation" (「埋め込み表現の意味適応による知識ベース語義曖昧性解消」ができるまで). Journal of Natural Language Processing, vol. 30, no. 3, pp. 1105-1109, September 2023. (in Japanese) [Article](https://www.jstage.jst.go.jp/article/jnlp/30/3/30_1105/_article/-char/ja)

## Others
* For a full list of publications including research papers written as part of office duties, refer to my [Google Scholar](https://scholar.google.co.jp/citations?user=ryX_tAEAAAAJ&hl=ja)

# Honors and Awards
* Mar. 2024: Best Paper Award (co-authored two papers), the 30th Annual Meeting of The Association for Natural Language Processing (NLP2024)
* Mar. 2023: Best Paper Award, the 29th Annual Meeting of The Association for Natural Language Processing (NLP2023)
* Mar. 2021: Best Paper Award, the 27th Annual Meeting of The Association for Natural Language Processing (NLP2021)
* Jan. 2018: Best Paper Award, IEICE Technical Committee on Natural Language Understanding and Models of Communication (NLC)

# Education
* Apr. 2018 - Dec. 2023: Ph.D. of Engineering (Computer Science), Tokyo Institute of Technology, Japan
    * Research field: Natural Language Processing
* Apr. 2007 - Mar. 2009: Master of Engineering, Tokyo University, Japan
    * Research field: Aerospace Engineering
* Apr. 2003 - Mar. 2007: Bachelor of Engineering, Nagoya University, Japan
    * Research field: Aerospace Engineering

# Work Experience
* Jul. 2024 - Present: National Institute of Advanced Industrial Science and Technology
    * Visiting Researcher
* Jan. 2024 - Present: Tokyo Institute of Technology
    * Part-time Researcher
* Mar. 2015 - Present: Hotto Link Inc.
    * Research Engineer
* Apr. 2009 - Feb. 2015: Mizuho Bank, Ltd. (seconded to [Mizuho-DL Financial Technology Co., Ltd.](https://www.mizuhobank.co.jp/fintec/index.html))
    * Financial Engineer

# Projects
* Oct. 2023 - Present: [Swallow LLM: large-scale language model research and development program](https://swallow-llm.github.io/index.en.html)

# Talks
* Aug. 2024: Paper Reading Seminar. Instruction-tuned LanguagInstruction-tuned Language Models are Better Knowledge Learnerse Models are Better Knowledge Learners. In: [最先端NLP勉強会 2024](https://sites.google.com/view/snlp-jp/home/2024). [Slide](https://speakerdeck.com/s_mizuki_nlp/instruction-tuned-language-models-are-better-knowledge-learners-in-acl-2024)
* Jan. 2024: LLM Seminar. 大規模言語モデルSwallow. In: [LLM勉強会](https://llm-jp.nii.ac.jp/resources/).
* Sep. 2023: LLM Seminar. Model imitationによるInstruction tuningのサーベイ. In: [LLM勉強会](https://llm-jp.nii.ac.jp/resources/).
* Mar. 2023: Invited talk. 埋め込み表現の意味適応による知識ベース語義曖昧性解消. In: [NLP Colloquium](https://nlp-colloquium-jp.github.io/).
* Mar. 2022: Workshop Lightning Talk. SNSを出典とする言語資源の公開にまつわるノウハウ. In: NLP2022 Co-located Workshop [日本語における評価用データセットの構築と利用性の向上](https://jedworkshop.github.io/jed2022/) (JED2022).
* Sep. 2021: Paper Reading Seminar. A Distributional Approach to Controlled Text Generation. In: [最先端NLP勉強会 2021](https://sites.google.com/view/snlp-jp/home/2021). [Slide](https://speakerdeck.com/s_mizuki_nlp/lun-wen-du-mihui-snlp2021-a-distributional-approach-to-controlled-text-generation)
* Sep. 2020: Paper Reading Seminar. ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators. In: [最先端NLP勉強会 2020](https://sites.google.com/view/snlp-jp/home/2020). [Slide](https://speakerdeck.com/s_mizuki_nlp/lun-wen-du-mihui-snlp2020-electra-pre-training-text-encoders-as-discriminators-rather-than-generators)
* Sep. 2019: Paper Reading Seminar. Ordered neurons: Integrating tree structures into recurrent neural networks. In: [最先端NLP勉強会 2019](https://sites.google.com/view/snlp-jp/home/2019). [Slide](https://speakerdeck.com/s_mizuki_nlp/lun-wen-du-mihui-snlp2019-ordered-neurons-integrating-tree-structures-into-recurrent-neural-networks)
* Aug. 2018: Paper Reading Seminar. Sequence-to-Action: End-to-End Semantic Graph Generation for Semantic Parsing. In: [最先端NLP勉強会 2018](https://sites.google.com/view/snlp-jp/home/2018). [Slide](https://speakerdeck.com/s_mizuki_nlp/lun-wen-du-mihui-snlp2018-sequence-to-action-end-to-end-semantic-graph-generation-for-semantic-parsing)
* Mar. 2016: Workshop. ソーシャルメディア分析サービスにおけるNLPに関する諸問題について. In: NLP2016 Co-located Workshop 論文に書かない（書けない）自然言語処理. 

# Membership
* The Association for Natural Language Processing (NLP)
* Information Processing Society of Japan (IPSJ)
* The Securities Analysts Association of Japan (SAAJ)
* [LLM-jp: LLM勉強会](https://llm-jp.nii.ac.jp/)

# Skills
* Japanese (native)
* English (fluent)
* Python, PyTorch (, R and C++)
* Elasticsearch
* Data science
